{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2af91aa4-7883-4ba9-923d-3f31d266648b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Homework 4\n",
    "\n",
    "```yaml\n",
    "Course:   DS 5001\n",
    "Module:   04 Lab\n",
    "Topic:    Homework 4\n",
    "Author:   Andrew Avitabile\n",
    "Date:     09 February 2024 (revised and improved)\n",
    "```\n",
    "\n",
    "**Purpose**: Demonstrate Hidden Markov Model applied to POS detection, including the Viterbi algorithm by hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c3394b-a78e-44c8-b911-5760f3ec3877",
   "metadata": {},
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f58366-7a11-400d-8005-a46173fb7cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import re\n",
    "import nltk\n",
    "import plotly_express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6c5513-f022-433b-8bf9-7d23c7a42df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1714b988-120d-42aa-a062-9f11cc1b1889",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"../../../env.ini\")\n",
    "data_home = config['DEFAULT']['data_home']\n",
    "output_dir = config['DEFAULT']['output_dir']\n",
    "local_lib = config['DEFAULT']['local_lib']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a40985-8f1c-49f7-9b1c-be6f55303603",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files = f'{data_home}/gutenberg/eliot-set'\n",
    "data_prefix = 'eliot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78cab8-587b-4da2-bcee-bc7975bfc907",
   "metadata": {},
   "outputs": [],
   "source": [
    "OHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f05959e-25d2-4b1c-a0f0-f23ed9fee6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(local_lib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4361e9ac-5c7a-44af-9b82-dc6058640835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textparser import TextParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033561d4-bd03-4808-94ea-4e902eff34dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c46aec-dabd-410f-980b-291e203bb6da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip_pats = [\n",
    "    r\"\\*\\*\\*\\s*START OF\",\n",
    "    r\"\\*\\*\\*\\s*END OF\"\n",
    "]\n",
    "\n",
    "# All are 'chap'and 'm'\n",
    "roman = '[IVXLCM]+'\n",
    "caps = \"[A-Z';, -]+\"\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf8706d-2f5c-48c0-8631-3c90fabe8bb8",
   "metadata": {},
   "source": [
    "# Register\n",
    "\n",
    "We get each file and add to a library `LIB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60bb0f-f874-4b95-aaf9-503956db258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file_list = sorted(glob(f\"{source_files}/*.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce1ed7-205d-429e-8d75-42c728181a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data = []\n",
    "for source_file_path in source_file_list:\n",
    "    # Splitting file path by \"\\\\\" for Windows paths or \"/\" for Unix-like paths\n",
    "    parts = source_file_path.split('\\\\') if '\\\\' in source_file_path else source_file_path.split('/')\n",
    "    \n",
    "    # Extracting book ID from the last part of the file name\n",
    "    book_id = int(parts[-1].split('-')[-1].split('.')[0].replace('pg',''))\n",
    "    \n",
    "    # Extracting book title from the second-to-last part of the file name\n",
    "    book_title = parts[-1].split('-')[0].replace('_', ' ')\n",
    "    \n",
    "    # Appending book data tuple to book_data list\n",
    "    book_data.append((book_id, source_file_path, book_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4009e58-599e-45d9-b567-30e6c31f0658",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB = pd.DataFrame(book_data, columns=['book_id','source_file_path','raw_title'])\\\n",
    "    .set_index('book_id').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e718daa-cf39-4fb7-b41c-b59924be7fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f2c92-517f-4e56-8342-792974bab4da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "book_id = int(source_file_path.split('-')[-1].split('.')[0].replace('pg',''))\n",
    "book_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa20cf7-25c7-4f2c-98e5-ad3686e2e81d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    LIB['author'] = LIB.raw_title.apply(lambda x: ', '.join(x.split()[:2]))\n",
    "    LIB['title'] = LIB.raw_title.apply(lambda x: ' '.join(x.split()[2:]))\n",
    "    LIB = LIB.drop('raw_title', axis=1)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edef581-b3e9-441d-b652-b35cd3131089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LIB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e329df-88bb-4c18-8c64-31d4495ad864",
   "metadata": {},
   "source": [
    "## Save Chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d77e2-6cfd-428a-8985-97b46667cdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB['chap_regex'] = LIB.index.map(pd.Series({x[0]:x[1] for x in ohco_pat_list}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7b598-6f06-4e93-9c3b-6bc5804da313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LIB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d657c2-09f0-4ea5-8803-edefe6f86bf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tokenize Corpus\n",
    "\n",
    "We tokenize each book and add each `TOKENS` table to a list to be concatenated into a single `CORPUS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd6efec-3626-4e92-85e1-e8d65f4f3660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_collection(LIB):\n",
    "\n",
    "    clip_pats = [\n",
    "        r\"\\*\\*\\*\\s*START OF\",\n",
    "        r\"\\*\\*\\*\\s*END OF\"\n",
    "    ]\n",
    "\n",
    "    books = []\n",
    "    for book_id in LIB.index:\n",
    "\n",
    "        # Announce\n",
    "        print(\"Tokenizing\", book_id, LIB.loc[book_id].title)\n",
    "\n",
    "        # Define vars\n",
    "        chap_regex = LIB.loc[book_id].chap_regex\n",
    "        ohco_pats = [('chap', chap_regex, 'm')]\n",
    "        src_file_path = LIB.loc[book_id].source_file_path\n",
    "\n",
    "        # Create object\n",
    "        text = TextParser(src_file_path, ohco_pats=ohco_pats, clip_pats=clip_pats, use_nltk=True)\n",
    "\n",
    "        # Define parameters\n",
    "        text.verbose = True\n",
    "        text.strip_hyphens = True\n",
    "        text.strip_whitespace = True\n",
    "\n",
    "        # Parse\n",
    "        text.import_source().parse_tokens();\n",
    "\n",
    "        # Name things\n",
    "        text.TOKENS['book_id'] = book_id\n",
    "        text.TOKENS = text.TOKENS.reset_index().set_index(['book_id'] + text.OHCO)\n",
    "\n",
    "        # Add to list\n",
    "        books.append(text.TOKENS)\n",
    "        \n",
    "    # Combine into a single dataframe\n",
    "    CORPUS = pd.concat(books).sort_index()\n",
    "\n",
    "    # Clean up\n",
    "    del(books)\n",
    "    del(text)\n",
    "        \n",
    "    print(\"Done\")\n",
    "        \n",
    "    return CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d94e9-f5f6-4b68-8079-4c8637b6c8ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CORPUS = tokenize_collection(LIB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188e14d6-0c5c-4f76-836a-ce22f3b3b194",
   "metadata": {},
   "source": [
    "## Extract some features for `LIB`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7d096-6e66-43db-9877-1cd64106d7d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LIB['book_len'] = CORPUS.groupby('book_id').term_str.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a1ae1-a4c5-4d25-8d41-7eac31ce47db",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB.sort_values('book_len')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccc0240-cfe8-4861-a56e-247b1725ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB['n_chaps'] = CORPUS.reset_index()[['book_id','chap_id']]\\\n",
    "    .drop_duplicates()\\\n",
    "    .groupby('book_id').chap_id.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef2b44d-786b-4ffb-a351-316da69aafad",
   "metadata": {},
   "source": [
    "# Exract VOCAB\n",
    "\n",
    "Extract a vocabulary from the CORPUS as a whole"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d412e2-d49f-4f58-a2a0-abf325434312",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Handle Anomalies\n",
    "\n",
    "NLTK's POS tagger is not perfect -- note the classification of punctuation as nouns, verbs, etc. We remove these from our corups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a357c-6095-41a8-bc33-36b584a804e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS[CORPUS.term_str == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb8e29c-0346-4e23-9716-858ff076d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS[CORPUS.term_str == ''].token_str.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085bb94-c4cc-495d-9a34-57244fda5485",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = CORPUS[CORPUS.term_str != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ffa0c0-d7dd-4b4d-aaef-e51732488772",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS['pos_group'] = CORPUS.pos.str[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac4ffbe-9581-4d30-9687-bc21741e4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0586877-37ee-4adb-ae82-8e25e5f39369",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB = CORPUS.term_str.value_counts().to_frame('n').sort_index()\n",
    "VOCAB.index.name = 'term_str'\n",
    "VOCAB['n_chars'] = VOCAB.index.str.len()\n",
    "VOCAB['p'] = VOCAB.n / VOCAB.n.sum()\n",
    "VOCAB['i'] = -np.log2(VOCAB.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb631aae-8e73-4dba-9338-b11853c4567a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ade7e9-fa6e-46cc-8c4d-1123ccb1d340",
   "metadata": {},
   "source": [
    "# Annotate VOCAB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd814198-64d9-4afe-9f27-10319106386c",
   "metadata": {},
   "source": [
    "## Get Max POS\n",
    "\n",
    "Get the most frequently associated part-of-speech category for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d6066-1cba-4a6c-b1b7-c5bdbc6d4663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VOCAB['max_pos'] = CORPUS[['term_str','pos']].value_counts().unstack(fill_value=0).idxmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dbba50-48df-4bde-b3d0-bb00caefcca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB['max_pos_group'] = CORPUS[['term_str','pos_group']].value_counts().unstack(fill_value=0).idxmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4dfa31-a5d5-4075-af68-2d7976393cde",
   "metadata": {},
   "source": [
    "## Compute POS ambiguity\n",
    "\n",
    "How many POS categories are associated with each word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3151e32-45c6-4c16-8a20-4a5ae8f0aa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB['n_pos_group'] = CORPUS[['term_str','pos_group']].value_counts().unstack().count(1)\n",
    "VOCAB['cat_pos_group'] = CORPUS[['term_str','pos_group']].value_counts().to_frame('n').reset_index()\\\n",
    "    .groupby('term_str').pos_group.apply(lambda x: set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e47c8c-f683-4973-a2fd-65342109c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB['n_pos'] = CORPUS[['term_str','pos']].value_counts().unstack().count(1)\n",
    "VOCAB['cat_pos'] = CORPUS[['term_str','pos']].value_counts().to_frame('n').reset_index()\\\n",
    "    .groupby('term_str').pos.apply(lambda x: set(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7289538-d87e-4ce8-90e4-0c84cbe3f127",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281c2fe5-6400-4969-bbd4-66753f2e4c4a",
   "metadata": {},
   "source": [
    "## Add Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48aa76-c9ba-446d-a36c-b811847679eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = pd.DataFrame(nltk.corpus.stopwords.words('english'), columns=['term_str'])\n",
    "sw = sw.reset_index().set_index('term_str')\n",
    "sw.columns = ['dummy']\n",
    "sw.dummy = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78910492-a131-4c16-876b-cc233b175206",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB['stop'] = VOCAB.index.map(sw.dummy)\n",
    "VOCAB['stop'] = VOCAB['stop'].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16081fed-3c9d-4906-a95a-b7d7045d5945",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB[VOCAB.stop == 1].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea7206-7a11-4536-9d09-4e8f398f9a7e",
   "metadata": {},
   "source": [
    "## Interlude: Stopword Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51acd443-5f59-473c-8580-aa93f7ee01ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = VOCAB.groupby('stop').n_chars.mean()\n",
    "b = VOCAB.groupby('stop').n_pos.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad361ad2-7f7d-45f7-ba28-d655519ca49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([a,b], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a027a4-b814-4b4b-9606-5643cd417e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB.groupby('n_chars').n_pos.mean()\\\n",
    "    .sort_index().plot.bar(rot=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bdfc1d-2bde-411e-b69d-9559d0c258f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB[VOCAB.stop == True].sort_values('n_pos', ascending=False)[['n_pos','cat_pos']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e64cb66-2fc8-425e-864f-24147aab3911",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = CORPUS.merge(LIB.reset_index()[['book_id','author']], on='book_id')\\\n",
    "    .merge(VOCAB.reset_index()[['term_str', 'stop']], on='term_str')\\\n",
    "    .groupby(['author','stop']).agg('sum', numeric_only=True).unstack()\n",
    "X.columns = X.columns.droplevel(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8b967-2f1c-4932-845d-c3240be0260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X.T / X.T.sum()).T.style.background_gradient(axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb10b2fc-e5b9-4276-8f33-0a1293fce3af",
   "metadata": {},
   "source": [
    "## Add Stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f7467-b3da-4114-9ddd-ea6868368695",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer1 = PorterStemmer()\n",
    "VOCAB['stem_porter'] = VOCAB.apply(lambda x: stemmer1.stem(x.name), 1)\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer2 = SnowballStemmer(\"english\")\n",
    "VOCAB['stem_snowball'] = VOCAB.apply(lambda x: stemmer2.stem(x.name), 1)\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer3 = LancasterStemmer()\n",
    "VOCAB['stem_lancaster'] = VOCAB.apply(lambda x: stemmer3.stem(x.name), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d6afc-815d-4c46-ba58-0b92fc8226a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d604b2e-3b95-4ef6-8a51-81f65713406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB[VOCAB.stem_porter != VOCAB.stem_snowball]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d662aabf-4f3d-4920-80ac-b9dd4ff2add7",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da7818-d049-4c6e-b6ab-edb0d09fd79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = f'{output_dir}/{data_prefix}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04081cca-b085-490a-86cc-96b891cee335",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIB.to_csv(f'{out_path}-LIB-eliot-set.csv')\n",
    "VOCAB.to_csv(f'{out_path}-VOCAB-eliot-set.csv')\n",
    "CORPUS.to_csv(f'{out_path}-CORPUS-eliot-set.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f853a2-a50a-4b9e-8f6f-5e3046211454",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11c96f4-59b4-43f1-9dd4-9e7468f44641",
   "metadata": {},
   "source": [
    "## 1. What regular expression did you use to chunk _Middlemarch_ into chapters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205476cc-f5c1-406b-a777-b2cacd8162d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "roman = '[IVXLCM]+'\n",
    "caps = \"[A-Z';, -]+\"\n",
    "ohco_pat_list = [\n",
    "    (145,   rf\"(PRELUDE|CHAPTER\\s+{roman}+)\"), #This one is for Middlemarch\n",
    "    (507,   rf'Chapter\\s{roman}+'),\n",
    "    (6688,  rf'Chapter\\s{roman}+')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dfeb16-4785-434e-81d7-a7753a6b7953",
   "metadata": {},
   "source": [
    "## 2. What is the title of the book that has the most tokens? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efde099-c9c1-4792-ae22-d0efd3df8876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_token_num_by_book = CORPUS.groupby('book_id').apply(lambda x: x.index.get_level_values('token_num').max())\n",
    "max_token_num_by_book_df = max_token_num_by_book.reset_index(name='max_token_num')\n",
    "max_token_num_by_book_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11c5c62-23da-454e-bb79-9f72e9d7ff7f",
   "metadata": {},
   "source": [
    "ADAM BEDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e704511-b68a-4ecf-bb86-c8639d73cbf4",
   "metadata": {},
   "source": [
    "## 3. How many chapter level chunks are there in this novel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8783c1bf-e2c2-4c5b-a1d5-0ea625d50911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chap_id_count_by_book = CORPUS.reset_index().groupby('book_id')['chap_id'].nunique().reset_index(name='unique_chap_id_count')\n",
    "chap_id_count_by_book"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0d8b39-74ab-4fd1-98ff-730e6afe08c7",
   "metadata": {},
   "source": [
    "61"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf0430-092c-4279-a2ca-1ec6863ea561",
   "metadata": {},
   "source": [
    "## 4. Among the three stemming algorithms -- Porter, Lancaster, and Snowball --  which is the most aggressive, in terms of the number of words associated with each stem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920a71ab-341a-426a-96fd-31462dd5aef2",
   "metadata": {},
   "source": [
    "Lancaster is the most aggressive stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4705e5-d49a-4c65-880e-1ece6523c475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "term_counts_porter = VOCAB.groupby(['stem_porter']).size().reset_index(name='term_count')\n",
    "term_counts_porter['term_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0312e935-2634-4bd9-b2e3-98470667239c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "term_counts_lancaster = VOCAB.groupby(['stem_lancaster']).size().reset_index(name='term_count')\n",
    "term_counts_lancaster['term_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375254de-c3c4-450b-a0d5-062ee132160e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "term_counts_snowball = VOCAB.groupby(['stem_snowball']).size().reset_index(name='term_count')\n",
    "term_counts_snowball['term_count'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a92a038-8384-40b8-880e-5e6769d53d65",
   "metadata": {},
   "source": [
    "## 5. Using the most aggressive stemmer from the previous question, what is the stem with the most associated terms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99bbd19-b981-45e9-a50f-d759b23052ed",
   "metadata": {},
   "source": [
    "\"cont\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16549e0a-b8dc-4774-bb87-8e820c038ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "term_counts_lancaster[term_counts_lancaster['term_count'] == 34]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
