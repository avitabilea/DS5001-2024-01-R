{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914653a4-657b-4916-b374-70646f7856e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Homework 5\n",
    "\n",
    "```yaml\n",
    "Course:    DS 5001\n",
    "Module:    M05 Homework\n",
    "Author:    Andrew Avitabile\n",
    "Date:      18 February 2024\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347bf09c-f2a7-40d3-8945-ad1955fbbc6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b7ad681-1a29-42e1-81bd-55ccb7836654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856ad1e5-1e66-4df6-b8cd-1096b1a91b6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../../../env.ini\")\n",
    "data_home = config['DEFAULT']['data_home'] \n",
    "output_dir = config['DEFAULT']['output_dir']\n",
    "data_prefix = 'austen-melville'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc8476d3-48e6-4caf-8965-d0a7fb646011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OHCO = ['book_id', 'chap_num', 'para_num', 'sent_num', 'token_num']\n",
    "bags = dict(\n",
    "    SENTS = OHCO[:4],\n",
    "    PARAS = OHCO[:3],\n",
    "    CHAPS = OHCO[:2],\n",
    "    BOOKS = OHCO[:1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d6b39-f7d9-4dd4-af95-85f9f1e1a2bb",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b5dd6-da5c-4efb-9a39-70d10a5076e8",
   "metadata": {},
   "source": [
    "## Import tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "554ba557-ecc0-4a54-a759-9020c4fedfb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LIB = pd.read_csv(f\"{output_dir}/{data_prefix}-LIB.csv\").set_index('book_id')\n",
    "CORPUS = pd.read_csv(f'{output_dir}/{data_prefix}-TOKEN.csv').set_index(OHCO).dropna()\n",
    "VOCAB = pd.read_csv(f'{output_dir}/{data_prefix}-VOCAB.csv').set_index('term_str').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c546bfc9-8d6e-4c3b-b9f5-bee42c213df9",
   "metadata": {},
   "source": [
    "## Function to compute TFIDF\n",
    "\n",
    "The function takes the inputs:\n",
    "- CORPUS: The name of the CORPUS\n",
    "- bag: OCHO-level [SENTS, PARAS, CHAPS, BOOKS]\n",
    "- tf_method = [sum, max, log, raw, double_norm, binary]\n",
    "- idf_method = [standard, max, smooth]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1501746e-48fb-4b64-b1f9-5c010d1e42e6",
   "metadata": {},
   "source": [
    "### 1. Show the function you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d297d8fa-a466-4167-a05d-6be2bea4783d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_tfidf(CORPUS, bag, tf_method, idf_method):\n",
    "    \n",
    "    # Compute bag-of-words representation\n",
    "    BOW = CORPUS.groupby(bags[bag]+['term_str']).term_str.count().to_frame('n') \n",
    "    \n",
    "    # Create document-term matrix\n",
    "    DTCM = BOW.n.unstack(fill_value=0)\n",
    "    \n",
    "    # Compute statistics for each document\n",
    "    DOC = DTCM.sum(1).to_frame('n_tokens')\n",
    "    DOC['n_types'] = DTCM.astype('bool').sum(1)\n",
    "    DOC['pkr'] = DOC.n_types / DOC.n_tokens\n",
    "    DOC = DOC.join(LIB[['author','title']])\n",
    "    \n",
    "    # Display top documents based on type-token ratio\n",
    "    DOC.sort_values('pkr').head(20).style.background_gradient(cmap='YlGnBu')\n",
    "    \n",
    "    # Print chosen TF method\n",
    "    print('TF method:', tf_method)\n",
    "    # Compute term frequency (TF) based on selected method\n",
    "    if tf_method == 'sum':\n",
    "        TF = DTCM.T / DTCM.T.sum()\n",
    "    elif tf_method == 'max':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "    elif tf_method == 'log':\n",
    "        TF = np.log2(1 + DTCM.T)\n",
    "    elif tf_method == 'raw':\n",
    "        TF = DTCM.T\n",
    "    elif tf_method == 'double_norm':\n",
    "        TF = DTCM.T / DTCM.T.max()\n",
    "    elif tf_method == 'binary':\n",
    "        TF = DTCM.T.astype('bool').astype('int')\n",
    "    TF = TF.T\n",
    "    \n",
    "    # Compute document frequency (DF) for each term\n",
    "    DF = DTCM.astype('bool').sum()\n",
    "    \n",
    "    # Compute total number of documents\n",
    "    N = DTCM.shape[0]\n",
    "    # Print chosen IDF method\n",
    "    print('IDF method:', idf_method)\n",
    "    # Compute inverse document frequency (IDF) based on selected method\n",
    "    if idf_method == 'standard':\n",
    "        IDF = np.log2(N / DF)\n",
    "    elif idf_method == 'max':\n",
    "        IDF = np.log2(DF.max() / DF)\n",
    "    elif idf_method == 'smooth':\n",
    "        IDF = np.log2((1 + N) / (1 + DF)) + 1\n",
    "    \n",
    "    # Compute TF-IDF matrix\n",
    "    TFIDF = TF * IDF\n",
    "    \n",
    "    return TFIDF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d07ec36-91f2-46cb-8462-9113649bf22c",
   "metadata": {},
   "source": [
    "### 2. What are the top 20 words in the corpus by TFIDF mean using the `max` count method and `book` as the bag?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37609050-d843-49ea-9835-4993d528e052",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF method: max\n",
      "IDF method: standard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "term_str\n",
       "elinor        0.035065\n",
       "pierre        0.031845\n",
       "vernon        0.026919\n",
       "marianne      0.021992\n",
       "emma          0.021686\n",
       "darcy         0.020000\n",
       "reginald      0.019154\n",
       "babbalanja    0.018803\n",
       "frederica     0.018637\n",
       "catherine     0.018472\n",
       "crawford      0.018391\n",
       "elliot        0.017670\n",
       "fanny         0.017492\n",
       "weston        0.017191\n",
       "media         0.016469\n",
       "israel        0.015808\n",
       "knightley     0.015733\n",
       "tilney        0.014315\n",
       "elton         0.014142\n",
       "bingley       0.013744\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run function\n",
    "TFIDF = compute_tfidf(CORPUS, bag='BOOKS', tf_method='max', idf_method='standard')\n",
    "\n",
    "#Compute mean TF-IDF scores for each term\n",
    "mean_tfidf = TFIDF.mean()\n",
    "\n",
    "#Sort terms by mean TF-IDF scores and select the top 20\n",
    "top_20_words = mean_tfidf.sort_values(ascending=False).head(20)\n",
    "top_20_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486b1b1-edc8-4932-adf9-1eae011abac8",
   "metadata": {},
   "source": [
    "### 3. What are the top 20 words in the corpus by TFIDF mean, if you using the `sum` count method and  `chapter` as the bag? Note, because of the greater number of bags, this will take longer to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5aa04c2-0e5d-4c66-975c-e080dd1d59aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF method: sum\n",
      "IDF method: standard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "term_str\n",
       "her             0.004280\n",
       "she             0.004266\n",
       "cosmopolitan    0.003664\n",
       "pierre          0.003448\n",
       "you             0.002706\n",
       "i               0.002623\n",
       "hypothetical    0.002579\n",
       "mr              0.002132\n",
       "boon            0.001957\n",
       "whale           0.001791\n",
       "mrs             0.001780\n",
       "charming        0.001767\n",
       "thou            0.001758\n",
       "and             0.001666\n",
       "my              0.001638\n",
       "lady            0.001638\n",
       "me              0.001617\n",
       "disciple        0.001602\n",
       "charitable      0.001556\n",
       "your            0.001508\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run function\n",
    "TFIDF = compute_tfidf(CORPUS, bag='CHAPS', tf_method='sum', idf_method='standard')\n",
    "\n",
    "#Compute mean TF-IDF scores for each term\n",
    "mean_tfidf = TFIDF.mean()\n",
    "\n",
    "#Sort terms by mean TF-IDF scores and select the top 20\n",
    "top_20_words = mean_tfidf.sort_values(ascending=False).head(20)\n",
    "top_20_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97cc65a-eee3-4d06-b0d9-46d2de1d8a3d",
   "metadata": {},
   "source": [
    "### 4. Characterize the general difference between the words in Question 3 and those in Question 2 in terms of part-of-speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da565c87-127e-43da-be38-38310e24865e",
   "metadata": {
    "tags": []
   },
   "source": [
    "The words in question 2 are all proper nouns, whereas those in question 3 are non-proper noun parts of speech (e.g., pronous: her, she, you, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a25ec46-b2b6-475a-9cd3-2168921453d2",
   "metadata": {},
   "source": [
    "### 5. Compute mean `TFIDF` for vocabularies conditioned on individual author, using *chapter* as the bag and `max` as the `TF` count method. Among the two authors, whose work has the most significant adjective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b3092b7-2cfa-47d0-88a0-d02b2bbbe3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF method: max\n",
      "IDF method: standard\n"
     ]
    }
   ],
   "source": [
    "#Run function\n",
    "TFIDF = compute_tfidf(CORPUS, bag='CHAPS', tf_method='max', idf_method='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fec83cca-855d-435f-b0fc-d5e9308429f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>p</th>\n",
       "      <th>i</th>\n",
       "      <th>max_pos</th>\n",
       "      <th>max_pos_group</th>\n",
       "      <th>n_pos_group</th>\n",
       "      <th>cat_pos_group</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>cat_pos</th>\n",
       "      <th>stop</th>\n",
       "      <th>stem_porter</th>\n",
       "      <th>stem_snowball</th>\n",
       "      <th>stem_lancaster</th>\n",
       "      <th>mean_tfidf</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term_str</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ugh</th>\n",
       "      <td>127</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>13.898738</td>\n",
       "      <td>JJ</td>\n",
       "      <td>JJ</td>\n",
       "      <td>5</td>\n",
       "      <td>{'VB', 'JJ', 'RB', 'NN', 'IN'}</td>\n",
       "      <td>8</td>\n",
       "      <td>{'VB', 'JJ', 'NNP', 'VBP', 'RB', 'NNS', 'NN', ...</td>\n",
       "      <td>0</td>\n",
       "      <td>ugh</td>\n",
       "      <td>ugh</td>\n",
       "      <td>ugh</td>\n",
       "      <td>0.012657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            n  n_chars         p          i max_pos max_pos_group  \\\n",
       "term_str                                                            \n",
       "ugh       127        3  0.000065  13.898738      JJ            JJ   \n",
       "\n",
       "          n_pos_group                   cat_pos_group  n_pos  \\\n",
       "term_str                                                       \n",
       "ugh                 5  {'VB', 'JJ', 'RB', 'NN', 'IN'}      8   \n",
       "\n",
       "                                                    cat_pos  stop stem_porter  \\\n",
       "term_str                                                                        \n",
       "ugh       {'VB', 'JJ', 'NNP', 'VBP', 'RB', 'NNS', 'NN', ...     0         ugh   \n",
       "\n",
       "         stem_snowball stem_lancaster  mean_tfidf  \n",
       "term_str                                           \n",
       "ugh                ugh            ugh    0.012657  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB['mean_tfidf'] = TFIDF.mean()\n",
    "VOCAB_JJ = VOCAB[VOCAB['max_pos_group'] == \"JJ\"]\n",
    "VOCAB_JJ.mean_tfidf.sort_values(ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc079a7e-d1ac-4fe6-b4f8-56560781df0f",
   "metadata": {},
   "source": [
    "\"ugh\" is the most significant adjective. Who wrote \"ugh\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c963f55-e671-4f29-a9da-565002952c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pos_tuple</th>\n",
       "      <th>pos</th>\n",
       "      <th>token_str</th>\n",
       "      <th>term_str</th>\n",
       "      <th>pos_group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book_id</th>\n",
       "      <th>chap_num</th>\n",
       "      <th>para_num</th>\n",
       "      <th>sent_num</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">21816</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">60</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">9</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>('\"Ugh,', 'JJ')</td>\n",
       "      <td>JJ</td>\n",
       "      <td>\"Ugh,</td>\n",
       "      <td>ugh</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>('ugh', 'IN')</td>\n",
       "      <td>IN</td>\n",
       "      <td>ugh</td>\n",
       "      <td>ugh</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">17</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>16</th>\n",
       "      <td>('ugh,', 'JJ')</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ugh,</td>\n",
       "      <td>ugh</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>('ugh,', 'NN')</td>\n",
       "      <td>NN</td>\n",
       "      <td>ugh,</td>\n",
       "      <td>ugh</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>('ugh!\"', 'NN')</td>\n",
       "      <td>NN</td>\n",
       "      <td>ugh!\"</td>\n",
       "      <td>ugh</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">66</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">68</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>6</th>\n",
       "      <td>('ugh,', 'JJ')</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ugh,</td>\n",
       "      <td>ugh</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>('ugh!', 'NN')</td>\n",
       "      <td>NN</td>\n",
       "      <td>ugh!</td>\n",
       "      <td>ugh</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">3</th>\n",
       "      <th>5</th>\n",
       "      <td>('ugh,', 'JJ')</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ugh,</td>\n",
       "      <td>ugh</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>('ugh,', 'JJ')</td>\n",
       "      <td>JJ</td>\n",
       "      <td>ugh,</td>\n",
       "      <td>ugh</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>('ugh!\"', 'NN')</td>\n",
       "      <td>NN</td>\n",
       "      <td>ugh!\"</td>\n",
       "      <td>ugh</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    pos_tuple pos token_str  \\\n",
       "book_id chap_num para_num sent_num token_num                                  \n",
       "21816   60       9        0        0          ('\"Ugh,', 'JJ')  JJ     \"Ugh,   \n",
       "                                   1            ('ugh', 'IN')  IN       ugh   \n",
       "                 17       3        16          ('ugh,', 'JJ')  JJ      ugh,   \n",
       "                                   17          ('ugh,', 'NN')  NN      ugh,   \n",
       "                                   18         ('ugh!\"', 'NN')  NN     ugh!\"   \n",
       "...                                                       ...  ..       ...   \n",
       "        66       68       2        6           ('ugh,', 'JJ')  JJ      ugh,   \n",
       "                                   7           ('ugh!', 'NN')  NN      ugh!   \n",
       "                          3        5           ('ugh,', 'JJ')  JJ      ugh,   \n",
       "                                   6           ('ugh,', 'JJ')  JJ      ugh,   \n",
       "                                   7          ('ugh!\"', 'NN')  NN     ugh!\"   \n",
       "\n",
       "                                             term_str pos_group  \n",
       "book_id chap_num para_num sent_num token_num                     \n",
       "21816   60       9        0        0              ugh        JJ  \n",
       "                                   1              ugh        IN  \n",
       "                 17       3        16             ugh        JJ  \n",
       "                                   17             ugh        NN  \n",
       "                                   18             ugh        NN  \n",
       "...                                               ...       ...  \n",
       "        66       68       2        6              ugh        JJ  \n",
       "                                   7              ugh        NN  \n",
       "                          3        5              ugh        JJ  \n",
       "                                   6              ugh        JJ  \n",
       "                                   7              ugh        NN  \n",
       "\n",
       "[127 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = CORPUS[CORPUS['term_str']==\"ugh\"]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c08f6-d3f2-4eb1-aded-36810e9e95f2",
   "metadata": {},
   "source": [
    "HERMAN MELVILLE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
